{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3AztkrMBPNCXv0IJECrt1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ji299/DLHW2/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 0) ä¾è³´ï¼ˆé¦–æ¬¡åŸ·è¡Œï¼‰\n",
        "#    - ç¬¬ä¸€æ¬¡è·‘ Colab æ™‚å»ºè­°æ•´æ®µç›´æ¥åŸ·è¡Œ\n",
        "# ===============================================================\n",
        "!pip -q install --upgrade pip cython wheel\n",
        "!pip -q install \"pycocotools @ git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\"\n",
        "!pip -q install torchmetrics==1.3.0 timm==0.9.12\n",
        "!pip -q install --quiet \"torchvision>=0.18\"\n",
        "\n",
        "# ===============================================================\n",
        "# 1) åŒ¯å…¥ + NumPy2 ç›¸å®¹å±¤\n",
        "# ===============================================================\n",
        "import os, json, time, warnings, numpy as np\n",
        "for d, n in {\"float\":\"float64\",\"int\":\"int64\",\"bool\":\"bool_\"}.items():\n",
        "    if not hasattr(np, d): setattr(np, d, getattr(np, n))\n",
        "\n",
        "from PIL import Image\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, datasets, models, ops as tv_ops\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from functools import partial\n",
        "from google.colab import drive\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"âœ… è£ç½® =\", device)\n",
        "\n",
        "# ===============================================================\n",
        "# 2) âš™ï¸ è¶…åƒæ•¸\n",
        "# ===============================================================\n",
        "IMG_SIZE, BATCH = 512, 8\n",
        "EPOCH_SEG, EPOCH_DET, EPOCH_CLS = 10, 40, 10\n",
        "LR_SEG, LR_DET, LR_CLS = 1e-3, 5e-4, 1e-4\n",
        "L_EWC_SEG_STAGE2, L_EWC_SEG_STAGE3, L_EWC_DET_STAGE3 = 2.0, 100.0, 50.0\n",
        "TH_CONF, TOPK_DET = 0.005, 256\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# ===============================================================\n",
        "# 3) Google Drive & è·¯å¾‘\n",
        "# ===============================================================\n",
        "drive.mount('/content/drive')\n",
        "DATA_DIR = \"/content/drive/MyDrive/DLHW2\"          # â† æ ¹æ“šéœ€æ±‚èª¿æ•´\n",
        "SAVE_DIR = \"/content/drive/MyDrive/DLHW2/models\"   # â† æ¨¡å‹å„²å­˜ç›®éŒ„\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "COCO_TRAIN_IMG  = f\"{DATA_DIR}/mini_coco_det/train/images\"\n",
        "COCO_TRAIN_JSON = f\"{DATA_DIR}/mini_coco_det/train/annotations/train300.json\"\n",
        "COCO_VAL_IMG    = f\"{DATA_DIR}/mini_coco_det/val/images\"\n",
        "COCO_VAL_JSON   = f\"{DATA_DIR}/mini_coco_det/val/annotations/val300.json\"\n",
        "\n",
        "VOC_TRAIN_IMG   = f\"{DATA_DIR}/mini_voc_seg/train/images\"\n",
        "VOC_TRAIN_MASK  = f\"{DATA_DIR}/mini_voc_seg/train/masks\"\n",
        "VOC_VAL_IMG     = f\"{DATA_DIR}/mini_voc_seg/val/images\"\n",
        "VOC_VAL_MASK    = f\"{DATA_DIR}/mini_voc_seg/val/masks\"\n",
        "\n",
        "CLS_TRAIN_DIR   = f\"{DATA_DIR}/mini_imagenette_160/train\"\n",
        "CLS_VAL_DIR     = f\"{DATA_DIR}/mini_imagenette_160/val\"\n",
        "\n",
        "# ===============================================================\n",
        "# 4) Datasetï¼ˆå« cat2idx ä¿®æ­£ï¼‰\n",
        "# ===============================================================\n",
        "class DetDataset(Dataset):\n",
        "    def __init__(self, img_dir, ann_file, img_size=512, grid=16, cat2idx=None):\n",
        "        self.img_dir, self.img_size, self.grid = img_dir, img_size, grid\n",
        "        with open(ann_file) as f: coco = json.load(f)\n",
        "\n",
        "        if cat2idx is None:\n",
        "            used = sorted({a[\"category_id\"] for a in coco[\"annotations\"]})\n",
        "            self.cat2idx = {cid:i for i,cid in enumerate(used)}\n",
        "        else:\n",
        "            self.cat2idx = cat2idx\n",
        "        self.num_classes = len(self.cat2idx)\n",
        "\n",
        "        self.id2name = {im[\"id\"]: im[\"file_name\"] for im in coco[\"images\"]}\n",
        "        self.img_ids = list(self.id2name.keys())\n",
        "\n",
        "        self.ann = {i: [] for i in self.img_ids}\n",
        "        for a in coco[\"annotations\"]:\n",
        "            if a[\"image_id\"] in self.ann and a[\"category_id\"] in self.cat2idx:\n",
        "                cid = self.cat2idx[a[\"category_id\"]]\n",
        "                self.ann[a[\"image_id\"]].append((cid, *a[\"bbox\"]))\n",
        "\n",
        "        self.t = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self): return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        iid = self.img_ids[idx]\n",
        "        img = Image.open(os.path.join(self.img_dir, self.id2name[iid])).convert(\"RGB\")\n",
        "        w0,h0 = img.size\n",
        "        img = self.t(img)\n",
        "\n",
        "        sx,sy = self.img_size/w0, self.img_size/h0\n",
        "        cell  = self.img_size/self.grid\n",
        "        tgt = torch.zeros((self.grid, self.grid, 1+4+self.num_classes))\n",
        "        for cid,x,y,w,h in self.ann[iid]:\n",
        "            cx,cy = (x+w/2)*sx, (y+h/2)*sy\n",
        "            gw,gh = w*sx, h*sy\n",
        "            gi,gj = int(cx/cell), int(cy/cell)\n",
        "            tgt[gj,gi,0] = 1.\n",
        "            tgt[gj,gi,1:5] = torch.tensor([cx/IMG_SIZE,cy/IMG_SIZE, gw/IMG_SIZE,gh/IMG_SIZE])\n",
        "            tgt[gj,gi,5+cid] = 1.\n",
        "        return img, tgt\n",
        "\n",
        "class SegDataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_dir, img_size=512):\n",
        "        self.img_dir, self.mask_dir, self.img_size = img_dir,mask_dir,img_size\n",
        "        self.imgs = sorted([f for f in os.listdir(img_dir)\n",
        "                            if f.lower().endswith((\"jpg\",\"jpeg\",\"png\"))])\n",
        "        self.t = transforms.Compose([\n",
        "            transforms.Resize((img_size,img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "        ])\n",
        "    def __len__(self): return len(self.imgs)\n",
        "    def _mask(self,fn):\n",
        "        stem = os.path.splitext(fn)[0]\n",
        "        for ext in (\".png\",\".jpg\",\".jpeg\",\".bmp\"):\n",
        "            p = os.path.join(self.mask_dir, stem+ext)\n",
        "            if os.path.isfile(p): return p\n",
        "        raise FileNotFoundError\n",
        "    def __getitem__(self,idx):\n",
        "        fn  = self.imgs[idx]\n",
        "        img = Image.open(os.path.join(self.img_dir,fn)).convert(\"RGB\")\n",
        "        msk = Image.open(self._mask(fn)).convert(\"L\")\n",
        "        img = self.t(img)\n",
        "        msk = msk.resize((self.img_size,self.img_size), Image.NEAREST)\n",
        "        msk = torch.from_numpy(np.array(msk,np.int64))\n",
        "        msk[msk>=21] = 0\n",
        "        return img, msk\n",
        "\n",
        "# -------- Image Classification Dataset --------\n",
        "t_cls = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "cls_train = datasets.ImageFolder(CLS_TRAIN_DIR, t_cls)\n",
        "cls_val   = datasets.ImageFolder(CLS_VAL_DIR , t_cls)\n",
        "\n",
        "def col(b): return torch.stack([x[0] for x in b]), torch.stack([x[1] for x in b])\n",
        "\n",
        "det_train = DetDataset(COCO_TRAIN_IMG, COCO_TRAIN_JSON)\n",
        "det_val   = DetDataset(COCO_VAL_IMG , COCO_VAL_JSON , cat2idx=det_train.cat2idx)\n",
        "seg_train = SegDataset(VOC_TRAIN_IMG, VOC_TRAIN_MASK)\n",
        "seg_val   = SegDataset(VOC_VAL_IMG , VOC_VAL_MASK)\n",
        "\n",
        "det_tr_loader  = DataLoader(det_train,  BATCH, True,  drop_last=True, collate_fn=col)\n",
        "det_val_loader = DataLoader(det_val,    1,     False, collate_fn=col)\n",
        "seg_tr_loader  = DataLoader(seg_train,  BATCH, True,  drop_last=True)\n",
        "seg_val_loader = DataLoader(seg_val,    1,     False)\n",
        "cls_tr_loader  = DataLoader(cls_train,  BATCH, True,  drop_last=True)\n",
        "cls_val_loader = DataLoader(cls_val,    1,     False)\n",
        "\n",
        "# ===============================================================\n",
        "# 5) å–®é ­æ¨¡å‹å®šç¾©\n",
        "# ===============================================================\n",
        "class Unified(nn.Module):\n",
        "    def __init__(self, n_det, n_seg, n_cls, grid=16):\n",
        "        super().__init__()\n",
        "        self.grid = grid\n",
        "        mbv3 = models.mobilenet_v3_small(\n",
        "            weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
        "        self.backbone = mbv3.features\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Conv2d(576,160,1), nn.BatchNorm2d(160), nn.ReLU())\n",
        "        ch = (n_det+5) + n_seg + n_cls\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(160,160,3,1,1), nn.ReLU(),\n",
        "            nn.Conv2d(160,ch,1))\n",
        "        self.n_det, self.n_seg, self.n_cls = n_det, n_seg, n_cls\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.neck(self.backbone(x))\n",
        "        out = self.head(f)\n",
        "        det, seg, cls = torch.split(out,\n",
        "                                    [self.n_det+5, 21, self.n_cls], dim=1)\n",
        "        seg = F.interpolate(seg, (IMG_SIZE,IMG_SIZE),\n",
        "                            mode='bilinear', align_corners=False)\n",
        "        cls = cls.mean(dim=[2,3])\n",
        "        return det, seg, cls\n",
        "\n",
        "# -------- å…¨åŸŸåƒæ•¸è¨ˆæ•¸å·¥å…· --------\n",
        "def count_parameters(net):\n",
        "    return sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
        "\n",
        "# -------- å»ºç«‹æ¨¡å‹ä¸¦å°å‡ºåƒæ•¸é‡ --------\n",
        "model = Unified(det_train.num_classes, 21, len(cls_train.classes)).to(device)\n",
        "num_params = count_parameters(model)\n",
        "print(f\"ğŸ”¢ æ¨¡å‹ç¸½åƒæ•¸é‡ï¼š{num_params/1e6:.2f} M\")\n",
        "\n",
        "if num_params > 8_000_000:\n",
        "    print(\"âš ï¸ è­¦å‘Šï¼šåƒæ•¸è¶…é 8Mï¼Œè«‹ç°¡åŒ–æ¨¡å‹æ¶æ§‹\")\n",
        "else:\n",
        "    print(\"âœ” æ¨¡å‹åƒæ•¸æ•¸é‡ç¬¦åˆé™åˆ¶ï¼ˆ< 8Mï¼‰\")\n",
        "\n",
        "print(f\"ğŸ”¢ åµæ¸¬é¡åˆ¥æ•¸ = {det_train.num_classes}\")\n",
        "\n",
        "# ===============================================================\n",
        "# 6) éšæ®µ 1ï¼šèªæ„åˆ†å‰²è¨“ç·´\n",
        "# ===============================================================\n",
        "loss_seg = nn.CrossEntropyLoss()\n",
        "opt_seg  = torch.optim.AdamW(model.parameters(), lr=LR_SEG)\n",
        "for ep in range(1, EPOCH_SEG+1):\n",
        "    model.train()\n",
        "    tot = 0.\n",
        "    for img, msk in seg_tr_loader:\n",
        "        img, msk = img.to(device), msk.to(device)\n",
        "        _, seg_out, _ = model(img)\n",
        "        l = loss_seg(seg_out, msk)\n",
        "        opt_seg.zero_grad()\n",
        "        l.backward()\n",
        "        opt_seg.step()\n",
        "        tot += l.item()\n",
        "    print(f\"[Seg] {ep}/{EPOCH_SEG} loss={tot/len(seg_tr_loader):.4f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def calc_mIoU(loader):\n",
        "    inter = union = 0\n",
        "    model.eval()\n",
        "    for img, msk in loader:\n",
        "        img, msk = img.to(device), msk.to(device)\n",
        "        _, seg_out, _ = model(img)\n",
        "        pred = seg_out.argmax(1)\n",
        "        inter += (pred == msk).sum().item()\n",
        "        union += msk.numel()\n",
        "    return inter / union\n",
        "\n",
        "mIoU_base = calc_mIoU(seg_val_loader)\n",
        "print(f\"mIoU_base = {mIoU_base:.3f}\")\n",
        "\n",
        "# -------- è¨ˆç®—èªæ„åˆ†å‰²çš„ Fisher è³‡è¨ŠçŸ©é™£ --------\n",
        "fisher_seg, theta_seg = {}, {}\n",
        "for n, p in model.named_parameters():\n",
        "    fisher_seg[n] = torch.zeros_like(p)\n",
        "    theta_seg[n] = p.detach().clone()\n",
        "for img, msk in seg_tr_loader:\n",
        "    img, msk = img.to(device), msk.to(device)\n",
        "    model.zero_grad()\n",
        "    loss_seg(model(img)[1], msk).backward()\n",
        "    for n, p in model.named_parameters():\n",
        "        fisher_seg[n] += p.grad.detach()**2\n",
        "for n in fisher_seg:\n",
        "    fisher_seg[n] /= len(seg_tr_loader)\n",
        "\n",
        "# ===============================================================\n",
        "# 7) éšæ®µ 2ï¼šç›®æ¨™åµæ¸¬ + EWC\n",
        "# ===============================================================\n",
        "def split_det(t):\n",
        "    conf = t[:,:1]\n",
        "    bbox = t[:,1:5].sigmoid().clamp_min(1e-3)\n",
        "    cls  = t[:,5:]\n",
        "    return conf, bbox, cls\n",
        "\n",
        "loss_conf    = partial(tv_ops.sigmoid_focal_loss, gamma=1.0)\n",
        "loss_bbox    = nn.SmoothL1Loss(beta=0.1)\n",
        "loss_cls_det = nn.BCEWithLogitsLoss()\n",
        "opt_det      = torch.optim.AdamW(model.parameters(), lr=LR_DET)\n",
        "\n",
        "for ep in range(1, EPOCH_DET+1):\n",
        "    model.train()\n",
        "    tot = 0.\n",
        "    for imgs, tgts in det_tr_loader:\n",
        "        imgs, tgts = imgs.to(device), tgts.to(device)\n",
        "        det_out, _, _ = model(imgs)\n",
        "        conf, bbox, cls = split_det(det_out)\n",
        "\n",
        "        # å‰ 10 å€‹ epoch ç”¨ BCE warm-up\n",
        "        if ep <= 10:\n",
        "            Lc = nn.BCEWithLogitsLoss()(\n",
        "                conf, tgts[...,0].unsqueeze(1))\n",
        "        else:\n",
        "            Lc = loss_conf(\n",
        "                conf, tgts[...,0].unsqueeze(1), reduction='mean')\n",
        "\n",
        "        pos = tgts[...,0] > 0\n",
        "        if pos.sum():\n",
        "            Lb = loss_bbox(\n",
        "                bbox.permute(0,2,3,1)[pos],\n",
        "                tgts[...,1:5][pos])\n",
        "            Ld = loss_cls_det(\n",
        "                cls.permute(0,2,3,1)[pos],\n",
        "                tgts[...,5:][pos])\n",
        "        else:\n",
        "            Lb = Ld = torch.tensor(0., device=device)\n",
        "\n",
        "        # EWC æ­£å‰‡åŒ–é …\n",
        "        ewc = sum(\n",
        "            (fisher_seg[n] * (p - theta_seg[n])**2).sum()\n",
        "            for n, p in model.named_parameters()\n",
        "        )\n",
        "        loss = Lc + Lb + Ld + L_EWC_SEG_STAGE2 * ewc\n",
        "        opt_det.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_det.step()\n",
        "        tot += loss.item()\n",
        "    if ep == 1 or ep % 20 == 0:\n",
        "        print(f\"[Det] {ep}/{EPOCH_DET} loss={tot/len(det_tr_loader):.4f}\")\n",
        "\n",
        "# -------- è¨ˆç®— mAP_base --------\n",
        "@torch.no_grad()\n",
        "def eval_map(metric, loader):\n",
        "    model.eval()\n",
        "    for imgs, tgts in loader:\n",
        "        imgs, tgts = imgs.to(device), tgts.to(device)\n",
        "        det_out, _, _ = model(imgs)\n",
        "        conf, bbox, cls = split_det(det_out)\n",
        "\n",
        "        preds, gts = [], []\n",
        "        for b in range(imgs.size(0)):\n",
        "            conf_s = conf[b].sigmoid()[0]\n",
        "            cls_s  = cls[b].sigmoid()\n",
        "            max_cls, max_lbl = cls_s.max(0)\n",
        "            score_map = conf_s * max_cls\n",
        "            flat = score_map.flatten()\n",
        "            k = min(TOPK_DET, flat.numel())\n",
        "            vals, idxs = torch.topk(flat, k)\n",
        "            ys, xs = idxs//model.grid, idxs%model.grid\n",
        "\n",
        "            p_boxes, p_scores, p_labels = [], [], []\n",
        "            for sc, j, i in zip(vals, ys, xs):\n",
        "                if sc < TH_CONF: continue\n",
        "                cx, cy, w, h = (bbox[b,:,j,i] * IMG_SIZE).tolist()\n",
        "                x1, y1 = max(0, cx-w/2), max(0, cy-h/2)\n",
        "                x2, y2 = min(IMG_SIZE, cx+w/2), min(IMG_SIZE, cy+h/2)\n",
        "                p_boxes.append([x1,y1,x2,y2])\n",
        "                p_scores.append(sc.item())\n",
        "                p_labels.append(max_lbl[j,i].item())\n",
        "            if p_boxes:\n",
        "                boxes  = torch.tensor(p_boxes, device=device)\n",
        "                scores = torch.tensor(p_scores, device=device)\n",
        "                keep   = tv_ops.nms(boxes, scores, 0.5)\n",
        "                preds.append({\n",
        "                    \"boxes\": boxes[keep],\n",
        "                    \"scores\": scores[keep],\n",
        "                    \"labels\": torch.tensor(p_labels, device=device)[keep]\n",
        "                })\n",
        "            else:\n",
        "                preds.append({\n",
        "                    \"boxes\": torch.tensor([[0,0,1,1]], device=device),\n",
        "                    \"scores\": torch.tensor([1e-6], device=device),\n",
        "                    \"labels\": torch.tensor([0], device=device)\n",
        "                })\n",
        "\n",
        "            # å»ºç«‹ ground-truth\n",
        "            g_boxes, g_labels = [], []\n",
        "            for j in range(model.grid):\n",
        "                for i in range(model.grid):\n",
        "                    if tgts[b,j,i,0] == 1:\n",
        "                        cx, cy, w, h = (tgts[b,j,i,1:5] * IMG_SIZE).tolist()\n",
        "                        x1, y1 = max(0, cx-w/2), max(0, cy-h/2)\n",
        "                        x2, y2 = min(IMG_SIZE, cx+w/2), min(IMG_SIZE, cy+h/2)\n",
        "                        g_boxes.append([x1,y1,x2,y2])\n",
        "                        g_labels.append(tgts[b,j,i,5:].argmax().item())\n",
        "            if not g_boxes:\n",
        "                g_boxes, g_labels = [[0,0,1,1]], [0]\n",
        "            gts.append({\n",
        "                \"boxes\": torch.tensor(g_boxes, device=device),\n",
        "                \"labels\": torch.tensor(g_labels, device=device)\n",
        "            })\n",
        "\n",
        "        metric.update(preds, gts)\n",
        "\n",
        "metric_base = MeanAveragePrecision(iou_type='bbox', iou_thresholds=[0.5])\n",
        "eval_map(metric_base, det_val_loader)\n",
        "mAP_base = metric_base.compute()['map_50'].item()\n",
        "print(f\"mAP_base = {mAP_base:.4f}\")\n",
        "\n",
        "# -------- è¨ˆç®—åµæ¸¬çš„ Fisher è³‡è¨ŠçŸ©é™£ --------\n",
        "fisher_det, theta_det = {}, {}\n",
        "for n, p in model.named_parameters():\n",
        "    fisher_det[n] = torch.zeros_like(p)\n",
        "    theta_det[n] = p.detach().clone()\n",
        "for imgs, tgts in det_tr_loader:\n",
        "    imgs, tgts = imgs.to(device), tgts.to(device)\n",
        "    model.zero_grad()\n",
        "    Lc = loss_conf(\n",
        "        split_det(model(imgs)[0])[0],\n",
        "        tgts[...,0].unsqueeze(1),\n",
        "        reduction='mean'\n",
        "    )\n",
        "    Lc.backward()\n",
        "    for n, p in model.named_parameters():\n",
        "        fisher_det[n] += p.grad.detach()**2\n",
        "for n in fisher_det:\n",
        "    fisher_det[n] /= len(det_tr_loader)\n",
        "\n",
        "# ===============================================================\n",
        "# â€”â€” æ–°å¢ï¼šå–®ä»»å‹™åˆ†é¡ Baseline è¨“ç·´èˆ‡è©•ä¼°ï¼ˆTop1_baseï¼‰\n",
        "# ===============================================================\n",
        "from torchvision.models import MobileNet_V3_Small_Weights\n",
        "\n",
        "# 1) å»ºç«‹å–®ç¨åˆ†é¡ç¶²è·¯\n",
        "cls_net = models.mobilenet_v3_small(\n",
        "    weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1\n",
        ")\n",
        "cls_net.classifier[3] = nn.Linear(\n",
        "    cls_net.classifier[3].in_features,\n",
        "    len(cls_train.classes)\n",
        ")\n",
        "cls_net = cls_net.to(device)\n",
        "\n",
        "# 2) å–®ä»»å‹™åˆ†é¡è¨“ç·´\n",
        "opt_cls_base  = torch.optim.AdamW(cls_net.parameters(), lr=LR_CLS)\n",
        "loss_cls_base = nn.CrossEntropyLoss()\n",
        "for ep in range(1, EPOCH_CLS+1):\n",
        "    cls_net.train()\n",
        "    tot = 0.\n",
        "    for img, lbl in cls_tr_loader:\n",
        "        img, lbl = img.to(device), lbl.to(device)\n",
        "        logits = cls_net(img)\n",
        "        l = loss_cls_base(logits, lbl)\n",
        "        opt_cls_base.zero_grad()\n",
        "        l.backward()\n",
        "        opt_cls_base.step()\n",
        "        tot += l.item()\n",
        "    if ep == 1 or ep % 2 == 0:\n",
        "        print(f\"[Cls Base] {ep}/{EPOCH_CLS} loss={tot/len(cls_tr_loader):.4f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def top1_cls_only(net, loader):\n",
        "    net.eval()\n",
        "    correct = total = 0\n",
        "    for img, lbl in loader:\n",
        "        img, lbl = img.to(device), lbl.to(device)\n",
        "        preds = net(img).argmax(1)\n",
        "        correct += (preds == lbl).sum().item()\n",
        "        total   += lbl.size(0)\n",
        "    return correct / total\n",
        "\n",
        "Top1_base = top1_cls_only(cls_net, cls_val_loader)\n",
        "print(f\"Top1_base = {Top1_base:.3f}\")\n",
        "\n",
        "# ===============================================================\n",
        "# 8) éšæ®µ 3ï¼šåˆ†é¡ + EWC\n",
        "# ===============================================================\n",
        "loss_cls = nn.CrossEntropyLoss()\n",
        "opt_cls  = torch.optim.AdamW(model.parameters(), lr=LR_CLS)\n",
        "for ep in range(1, EPOCH_CLS+1):\n",
        "    model.train()\n",
        "    tot = 0.\n",
        "    for img, lbl in cls_tr_loader:\n",
        "        img, lbl = img.to(device), lbl.to(device)\n",
        "        _, _, logit = model(img)\n",
        "        l = loss_cls(logit, lbl)\n",
        "        # EWC æ­£å‰‡åŒ–ï¼šèªæ„åˆ†å‰² & åµæ¸¬\n",
        "        e_s = sum(\n",
        "            (fisher_seg[n] * (p - theta_seg[n])**2).sum()\n",
        "            for n, p in model.named_parameters()\n",
        "        )\n",
        "        e_d = sum(\n",
        "            (fisher_det[n] * (p - theta_det[n])**2).sum()\n",
        "            for n, p in model.named_parameters()\n",
        "        )\n",
        "        loss = l + L_EWC_SEG_STAGE3 * e_s + L_EWC_DET_STAGE3 * e_d\n",
        "        opt_cls.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_cls.step()\n",
        "        tot += l.item()\n",
        "    if ep == 1 or ep % 2 == 0:\n",
        "        print(f\"[Cls] {ep}/{EPOCH_CLS} loss={tot/len(cls_tr_loader):.4f}\")\n",
        "\n",
        "# ===============================================================\n",
        "# 9) æœ€çµ‚è©•ä¼°\n",
        "# ===============================================================\n",
        "@torch.no_grad()\n",
        "def top1(loader):\n",
        "    model.eval()\n",
        "    ok = tot = 0\n",
        "    for img, lbl in loader:\n",
        "        img, lbl = img.to(device), lbl.to(device)\n",
        "        _, _, logit = model(img)\n",
        "        ok += (logit.argmax(1) == lbl).sum().item()\n",
        "        tot += lbl.size(0)\n",
        "    return ok / tot\n",
        "\n",
        "metric_final = MeanAveragePrecision(iou_type='bbox', iou_thresholds=[0.5])\n",
        "eval_map(metric_final, det_val_loader)\n",
        "mAP_final = metric_final.compute()['map_50'].item()\n",
        "top1_acc  = top1(cls_val_loader)\n",
        "mIoU_final= calc_mIoU(seg_val_loader)\n",
        "\n",
        "print(\"\\n===== FINAL =====\")\n",
        "print(f\"Top-1    {top1_acc:.3f} (base {Top1_base:.3f}, drop {(Top1_base-top1_acc)/Top1_base*100:.2f}% )\")\n",
        "print(f\"mIoU     {mIoU_final:.3f} (drop {(mIoU_base-mIoU_final)/mIoU_base*100:.2f} %)\")\n",
        "print(f\"mAP      {mAP_final:.3f} (drop {(mAP_base-mAP_final)/mAP_base*100:.2f} %)\")\n",
        "\n",
        "# â€”â€” æª¢æŸ¥æ˜¯å¦ç¬¦åˆ Top-1 â‰¥ Top1_base âˆ’ 5% æ¢ä»¶\n",
        "if top1_acc >= Top1_base * 0.95:\n",
        "    print(\"âœ” Top-1 â‰¥ Top1_base âˆ’ 5%\")\n",
        "else:\n",
        "    print(\"âœ˜ Top-1 < Top1_base âˆ’ 5%\")\n",
        "\n",
        "# ===============================================================\n",
        "# 10) å»¶é²æ¸¬è©¦ & å­˜æª”ï¼ˆå·²æ”¹ç‚ºå­˜åˆ° Google Driveï¼‰\n",
        "# ===============================================================\n",
        "model.eval()\n",
        "dum = torch.randn(1,3,IMG_SIZE,IMG_SIZE,device=device)\n",
        "with torch.no_grad():\n",
        "    for _ in range(10): _ = model(dum)\n",
        "    if device.type == \"cuda\": torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    for _ in range(100): _ = model(dum)\n",
        "    if device.type == \"cuda\": torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "print(f\"ğŸš€ latency {(t1-t0)/100*1000:.2f} ms\")\n",
        "\n",
        "save_path = f\"{SAVE_DIR}/your_model.pt\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"âœ” æ¨¡å‹å·²å„²å­˜ â†’ {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDiACksAmLXq",
        "outputId": "0361b268-6010-4b16-9b54-85741999ac7c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'pycocotools' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pycocotools'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'torchmetrics' candidate (version 1.3.0 at https://files.pythonhosted.org/packages/95/f4/07d76def72c02f0d93e5eec953fd3349b653af0e0b792276aeb5b3e6f7bf/torchmetrics-1.3.0-py3-none-any.whl (from https://pypi.org/simple/torchmetrics/) (requires-python:>=3.8))\n",
            "Reason for being yanked: <none given>\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m123.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m136.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13/13\u001b[0m [timm]\n",
            "\u001b[1A\u001b[2Kâœ… è£ç½® = cuda\n",
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.83M/9.83M [00:00<00:00, 116MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¢ æ¨¡å‹ç¸½åƒæ•¸é‡ï¼š1.27 M\n",
            "âœ” æ¨¡å‹åƒæ•¸æ•¸é‡ç¬¦åˆé™åˆ¶ï¼ˆ< 8Mï¼‰\n",
            "ğŸ”¢ åµæ¸¬é¡åˆ¥æ•¸ = 63\n",
            "[Seg] 1/10 loss=0.3485\n",
            "[Seg] 2/10 loss=0.1108\n",
            "[Seg] 3/10 loss=0.0800\n",
            "[Seg] 4/10 loss=0.0526\n",
            "[Seg] 5/10 loss=0.0351\n",
            "[Seg] 6/10 loss=0.0278\n",
            "[Seg] 7/10 loss=0.0237\n",
            "[Seg] 8/10 loss=0.0475\n",
            "[Seg] 9/10 loss=0.0626\n",
            "[Seg] 10/10 loss=0.0345\n",
            "mIoU_base = 0.975\n",
            "[Det] 1/40 loss=0.6481\n",
            "[Det] 20/40 loss=0.0867\n",
            "[Det] 40/40 loss=0.0570\n",
            "mAP_base = 0.0032\n",
            "[Cls Base] 1/10 loss=2.1775\n",
            "[Cls Base] 2/10 loss=1.7283\n",
            "[Cls Base] 4/10 loss=0.8834\n",
            "[Cls Base] 6/10 loss=0.5004\n",
            "[Cls Base] 8/10 loss=0.3096\n",
            "[Cls Base] 10/10 loss=0.2167\n",
            "Top1_base = 0.800\n",
            "[Cls] 1/10 loss=2.8546\n",
            "[Cls] 2/10 loss=2.2172\n",
            "[Cls] 4/10 loss=1.8514\n",
            "[Cls] 6/10 loss=1.4952\n",
            "[Cls] 8/10 loss=1.1631\n",
            "[Cls] 10/10 loss=0.9318\n",
            "\n",
            "===== FINAL =====\n",
            "Top-1    0.667 (base 0.800, drop 16.67% )\n",
            "mIoU     0.975 (drop 0.01 %)\n",
            "mAP      0.000 (drop 99.57 %)\n",
            "âœ˜ Top-1 < Top1_base âˆ’ 5%\n",
            "ğŸš€ latency 6.41 ms\n",
            "âœ” æ¨¡å‹å·²å„²å­˜ â†’ /content/drive/MyDrive/DLHW2/models/your_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "megns4HCkAOM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
